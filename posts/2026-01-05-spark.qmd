---
layout: post
title: "Essentials of Apache Spark"
date: 2026-01-05
categories: [distributed-computing, data-engineering, apache-spark]
execute:
  eval: false
---


## Apache Spark

Apache Spark was  developed by Matei Zaharia of University of California, Berkeley in 2009 and was donated to Apache Software Foundation in 2013.

![Apache Spark](../assets/img/03-Apache_Spark_logo-01.png)

Spark is a distributed computing framework that addressed bottlenecks in the traditional Hadoop based big data tools. Spark achieves performance improvement on the following fronts 

- In-memory computing 
- Partitioning 
- Fault-tolerant computing 
- Unified engine for big data ecosystem

### In-memory computing:

Spark works via in-memory computing that retains the data cached in the memory. This alone can yield 100-1000x speedups. Imagine a computation with many steps. At each step, the data must be read from the disk, loaded into memory (RAM), processed, intermediate results are written back. This operation must be repeated until the operation is completed. Clearly, the bottleneck is the repeated read, write cycle of intermediary data to hard disk. Spark eliminates most of this repetitive I/O on the disk by keeping the intermediate results in memory, dramatically accelerating performance. Spark intelligently manages the memory by keeping maximum of the partition data in memory (RAM). When the partition data exceeds the allocated in-memory, spark spills over the least frequently used partitions to the physical memory (disk) and quickly read back if and when the partition is needed.

### Abstractions in Spark

Spark developed its core abstraction called Resilient Distributed Datasets followed by the popular Dataset and DataFrame which are the latest higher level API implementations of Spark.

### Data Partitioning:

Spark partitions the data either by a variable in the data or arbitrarily by equal size. Partitioning by a variable of the data frame can be created using `partitionby()` method and arbitrary partitions can be created using `repartition()` method. Proper partitioning is critical for parallelism and optimized performance.

### Fault-tolerant computing:

Spark is fault tolerant since it can rapidly recreate the partition data on any failed nodes. Sparks tracks the lineage of the partition from the original data. If a node fails, spark rebuilds the lost data without ever stopping or restarting the process. This enabled horizontal scaling to handle petabytes of data reliably.

### Spark has an Unified Engine

Traditional Hadoop based solutions relied on different independent frameworks for specific processing of the data. For example for bigdata processing, there was MapReduce, for Stream Processing there was Strom or Samza, for SQL queries there was Hive or Impala. These resulted in a data silos, complex architectures and expensive data overheads. This is because the engineers had to master many tools and integrate into their system. Spark has the core engine and has several higher level APIs such as Spark SQL, Spark Streaming, Spark MLlib and Spark Graphx for querying, real-time data processing, machine learning and graph analytics all unified in the Spark framework, making it powerful modern data stack.

### Spark is Ployglot

Spark's core is written in Scala and APIs for Python (pyspark), Java, R (sparklyr) and SQL are available making it a powerful language across different data and statistical stacks. Spark documentation describes near identical performance across these languages.