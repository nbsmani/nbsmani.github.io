{"title":"A Data Pipeline is a distributed system. Start Architecting It Like One","markdown":{"yaml":{"layout":"post","title":"A Data Pipeline is a distributed system. Start Architecting It Like One","date":"2024-11-08","categories":["docker","data-engineering"],"execute":{"eval":false}},"headingText":"Containerizing the Application","containsRefs":false,"markdown":"\n\n\nIf your data pipeline has more than one component (e.g python and a database), it is already a distributed system. Treating it anything less is why pipelines break when moved from a local machine to production. \nTo build a system that works everywhere, one must consider **service discovery, configuration management and network isolation** right from the beginning. \n \nLet us a build production ready and automated data ingestion pipeline using docker. We will first build a simple python app that takes a csv and stores in a PostgreSQL database, then evolves into a multi-container architecture where a data-importer service automatically ingests data into a dedicated DBMS. \n\nFor this data pipeline, lets get [Paris AirBnB Listings](https://data.insideairbnb.com/france/ile-de-france/paris/2025-09-12/data/listings.csv.gz) data  and  build that data importer app. This app will take this csv data and store it into a database inside the PostgreSQL container. To make this app usable for any data, we will parametrize all the input variables. \n\n```python\n#data_importer.py\nimport os\nimport sys\nimport pandas as pd\nfrom sqlalchemy import create_engine,URL\n\n#input_variables\n\ninput_file = sys.argv[1]\nusername = sys.argv[2]\npassword = sys.argv[3]\nhost = sys.argv[4]\nport = sys.argv[5]\ndatabase = sys.argv[6]\ntable_name = sys.argv[7]\n\n#read the csv and store into a dataframe\ndf = pd.read_csv(input_file)\n\n#Create an engine to connect to the database\ndbms_url = URL.create(\n    \"postgresql\", #drivername is handled by SQLAlchemy. Never changes b/w envs. \n    username = username,\n    password = password,\n    host = host,\n    port = port,\n    database = database\n)\nengine = create_engine(dbms_url,echo = False)\n\n#We can directly push the df to the PostgresSQL server\ndf.to_sql(name = table_name, con = engine, if_exists = 'replace')\n```\n\nThis app takes a CSV file and creates a `URL` object with parameterized connection details. In isolated environments, Docker's DNS system resolves service names to container IPs automatically. Everything is parameterized for reuse.\n\n\nWe can containerize this app simply using a Dockerfile. Assuming this script is stored under `./src/data_importer.py` and the downloaded data is placed in `data/01_paris_airbnb_listings.csv`. We will need pandas, sqlalchemy and psycopg2 as dependencies. \n\nLet's create a `requirements.txt` in the current working directory that will be used to build the docker image.\n\n```txt\npandas==2.3.1\nnumpy==2.0.2\nSQLAlchemy==2.0.43\npsycopg2-binary==2.9.11\n```\n\nWe will use python 3.9-slim as a base image and install the dependencies copy the data and script. We will run the app inside the container and the arguments variables can be passed using the `CMD` operator.\n\n```Dockerfile\nFROM python:3.9-slim\nWORKDIR /data_importer\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY ./data ./src ./\nENTRYPOINT [\"python\",\"data_importer.py\"]\nCMD [\"01_paris_airbnb_listings.csv\",\"sqluser\",\"secret\",\"dbms\",\"5432\", \"global_data\",\"paris_airbnb\" ]\n\n```\n\nThe image can be built using. \n```bash\ndocker build -t data_importer .\n```\n\nThe above command will yield an image named `data_importer`.\n\n## Orchestrating with Docker Compose\n\nOnce the image is successfully built, we can create a distributed system using docker-compose. We will use PostgreSQL 14 image for database and create a database service `dbms`. We will use a named volume `pgdata` for data persistence and use a mounted volume to enable communication between the `data_importer` container. The mounted volume will be mapped between the working directory of the data_importer app and the physical location in the local machine. This way, the system can take any data inside the local data directory and import to the SQL database automatically and securely. We will use a bridge network named `telnet` to enable container-to -container communication. lets save these parameters in a `docker-compose.yml` in the current working directory.\n\n\n```yml\nservices :\n  dbms :\n    image : postgres:14\n    environment:\n      POSTGRES_DB: global_data\n      POSTGRES_USER: sqluser\n      POSTGRES_PASSWORD: secret\n    volumes :\n      - pgdata:/var/lib/postgresql/data\n      - ./data:/imported_data\n    ports :\n      - \"5432:5432\"\n    networks :\n      - telnet\n  \n  dataimporter:\n    image: data_importer\n    depends_on:\n      - dbms\n    networks:\n      - telnet\n    command: [\"01_paris_airbnb_listings.csv\",\"sqluser\",\"secret\",\"dbms\",\"5432\",\"global_data\",\"paris_airbnb_data\"]\n  \nvolumes :\n  pgdata :\n\nnetworks :\n  telnet :\n    driver : bridge\n```\n\n\n## Deployment\n\nThe complete ETL pipeline can be deployed using: \n\n```bash\ndocker-compose up -d\n```\n\nUpon successful execution, the data would be safely stored inside the `global_data` database in the `dbms` server. This can be verified using a docker exec command.\n```bash\ndocker exec -it sql_dbms_1 psql -U sqluser -d global_data  \n```\n\nWhich yields\n```bash\n\npsql (14.19 (Debian 14.19-1.pgdg13+1))  \nType \"help\" for help.  \n  \nglobal_data=# \\dt  \n             List of relations  \nSchema |       Name        | Type  |  Owner     \n--------+-------------------+-------+---------  \npublic | paris_airbnb_data | table | sqluser  \n(1 row)  \n  \nglobal_data=#\n```\n\nNote that the table_name is correctly named as `paris_airbnb_data` contary to `paris_airbnb` as mentioned in the `Dockerfile`. This suggests that the data is correctly passed from the argument variables defined in the command operator in the `docker-compose.yml`.\n\n## Summary\n\nIn this guide, we transformed a simple Python app into a production-ready data pipeline by adopting distributed systems principles. \n- **Service Discovery:** Containers communicate through Docker DNS (`dbms`) instead of hardcoded IPs.\n- **Configuration Management:** All parameters are externalized through CLI arguments and environment variables.\n- **Network Isolation:** Secure bridge network enables container-to-container communication.\n- **Data Persistence:** Named volumes ensure database survival across container restarts.\n- **Orchestration:** Docker Compose manages multi-service dependencies and lifecycle.\n\nTreating data pipelines as a distributed systems from the beginning eliminates the system breakdown across environments and creates infrastructure that scales from local development to production deployment.\n\nThe complete pipeline is available in [GitHub](https://github.com/nbsmani/containerized-etl-pipeline.git).","srcMarkdownNoYaml":"\n\n\nIf your data pipeline has more than one component (e.g python and a database), it is already a distributed system. Treating it anything less is why pipelines break when moved from a local machine to production. \nTo build a system that works everywhere, one must consider **service discovery, configuration management and network isolation** right from the beginning. \n \nLet us a build production ready and automated data ingestion pipeline using docker. We will first build a simple python app that takes a csv and stores in a PostgreSQL database, then evolves into a multi-container architecture where a data-importer service automatically ingests data into a dedicated DBMS. \n\nFor this data pipeline, lets get [Paris AirBnB Listings](https://data.insideairbnb.com/france/ile-de-france/paris/2025-09-12/data/listings.csv.gz) data  and  build that data importer app. This app will take this csv data and store it into a database inside the PostgreSQL container. To make this app usable for any data, we will parametrize all the input variables. \n\n```python\n#data_importer.py\nimport os\nimport sys\nimport pandas as pd\nfrom sqlalchemy import create_engine,URL\n\n#input_variables\n\ninput_file = sys.argv[1]\nusername = sys.argv[2]\npassword = sys.argv[3]\nhost = sys.argv[4]\nport = sys.argv[5]\ndatabase = sys.argv[6]\ntable_name = sys.argv[7]\n\n#read the csv and store into a dataframe\ndf = pd.read_csv(input_file)\n\n#Create an engine to connect to the database\ndbms_url = URL.create(\n    \"postgresql\", #drivername is handled by SQLAlchemy. Never changes b/w envs. \n    username = username,\n    password = password,\n    host = host,\n    port = port,\n    database = database\n)\nengine = create_engine(dbms_url,echo = False)\n\n#We can directly push the df to the PostgresSQL server\ndf.to_sql(name = table_name, con = engine, if_exists = 'replace')\n```\n\nThis app takes a CSV file and creates a `URL` object with parameterized connection details. In isolated environments, Docker's DNS system resolves service names to container IPs automatically. Everything is parameterized for reuse.\n\n## Containerizing the Application\n\nWe can containerize this app simply using a Dockerfile. Assuming this script is stored under `./src/data_importer.py` and the downloaded data is placed in `data/01_paris_airbnb_listings.csv`. We will need pandas, sqlalchemy and psycopg2 as dependencies. \n\nLet's create a `requirements.txt` in the current working directory that will be used to build the docker image.\n\n```txt\npandas==2.3.1\nnumpy==2.0.2\nSQLAlchemy==2.0.43\npsycopg2-binary==2.9.11\n```\n\nWe will use python 3.9-slim as a base image and install the dependencies copy the data and script. We will run the app inside the container and the arguments variables can be passed using the `CMD` operator.\n\n```Dockerfile\nFROM python:3.9-slim\nWORKDIR /data_importer\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY ./data ./src ./\nENTRYPOINT [\"python\",\"data_importer.py\"]\nCMD [\"01_paris_airbnb_listings.csv\",\"sqluser\",\"secret\",\"dbms\",\"5432\", \"global_data\",\"paris_airbnb\" ]\n\n```\n\nThe image can be built using. \n```bash\ndocker build -t data_importer .\n```\n\nThe above command will yield an image named `data_importer`.\n\n## Orchestrating with Docker Compose\n\nOnce the image is successfully built, we can create a distributed system using docker-compose. We will use PostgreSQL 14 image for database and create a database service `dbms`. We will use a named volume `pgdata` for data persistence and use a mounted volume to enable communication between the `data_importer` container. The mounted volume will be mapped between the working directory of the data_importer app and the physical location in the local machine. This way, the system can take any data inside the local data directory and import to the SQL database automatically and securely. We will use a bridge network named `telnet` to enable container-to -container communication. lets save these parameters in a `docker-compose.yml` in the current working directory.\n\n\n```yml\nservices :\n  dbms :\n    image : postgres:14\n    environment:\n      POSTGRES_DB: global_data\n      POSTGRES_USER: sqluser\n      POSTGRES_PASSWORD: secret\n    volumes :\n      - pgdata:/var/lib/postgresql/data\n      - ./data:/imported_data\n    ports :\n      - \"5432:5432\"\n    networks :\n      - telnet\n  \n  dataimporter:\n    image: data_importer\n    depends_on:\n      - dbms\n    networks:\n      - telnet\n    command: [\"01_paris_airbnb_listings.csv\",\"sqluser\",\"secret\",\"dbms\",\"5432\",\"global_data\",\"paris_airbnb_data\"]\n  \nvolumes :\n  pgdata :\n\nnetworks :\n  telnet :\n    driver : bridge\n```\n\n\n## Deployment\n\nThe complete ETL pipeline can be deployed using: \n\n```bash\ndocker-compose up -d\n```\n\nUpon successful execution, the data would be safely stored inside the `global_data` database in the `dbms` server. This can be verified using a docker exec command.\n```bash\ndocker exec -it sql_dbms_1 psql -U sqluser -d global_data  \n```\n\nWhich yields\n```bash\n\npsql (14.19 (Debian 14.19-1.pgdg13+1))  \nType \"help\" for help.  \n  \nglobal_data=# \\dt  \n             List of relations  \nSchema |       Name        | Type  |  Owner     \n--------+-------------------+-------+---------  \npublic | paris_airbnb_data | table | sqluser  \n(1 row)  \n  \nglobal_data=#\n```\n\nNote that the table_name is correctly named as `paris_airbnb_data` contary to `paris_airbnb` as mentioned in the `Dockerfile`. This suggests that the data is correctly passed from the argument variables defined in the command operator in the `docker-compose.yml`.\n\n## Summary\n\nIn this guide, we transformed a simple Python app into a production-ready data pipeline by adopting distributed systems principles. \n- **Service Discovery:** Containers communicate through Docker DNS (`dbms`) instead of hardcoded IPs.\n- **Configuration Management:** All parameters are externalized through CLI arguments and environment variables.\n- **Network Isolation:** Secure bridge network enables container-to-container communication.\n- **Data Persistence:** Named volumes ensure database survival across container restarts.\n- **Orchestration:** Docker Compose manages multi-service dependencies and lifecycle.\n\nTreating data pipelines as a distributed systems from the beginning eliminates the system breakdown across environments and creates infrastructure that scales from local development to production deployment.\n\nThe complete pipeline is available in [GitHub](https://github.com/nbsmani/containerized-etl-pipeline.git)."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../assets/styles.css"],"output-file":"2025-11-09-docker-data-pipeline.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cyborg","layout":"post","title":"A Data Pipeline is a distributed system. Start Architecting It Like One","date":"2024-11-08","categories":["docker","data-engineering"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}