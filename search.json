[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "As a highly motivated Data Engineer, I am dedicated to driving value and innovation within the organizations I collaborate with. Senior Data Engineer with a PhD in Engineering Sciences and 8+ years of experience designing and deploying scalable, containerized data pipelines into production. Expert in Python, SQL, PySpark, and modern orchestration (Airflow) and infrastructure (Docker, AWS) tools. Proven ability to architect full data lifecycle solutions from distributed processing on HPC/Spark clusters to workflow automation and cloud storage ensuring data integrity, reproducibility, and performance. Seeking to apply deep systems- engineering expertise to data infrastructure challenges in the tech industry. I am committed to delivering high-performance outcomes while pursuing personal and professional growth in a dynamic, results-driven environment.\n\nBig Data Technologies: Spark, Kafka\nCloud Platforms: AWS, GCP, Azure\nContainerization: Docker, Kubernetes\nLanguages: Python, SQL,R\n\nI enjoy solving complex data problems and optimizing performance for large-scale applications."
  },
  {
    "objectID": "posts/2026-01-14-spark-architecture.html",
    "href": "posts/2026-01-14-spark-architecture.html",
    "title": "Essentials of Spark Architecture",
    "section": "",
    "text": "Apache Spark is originally built on RDD abstraction and it is constructed in a well defined layered architecture. DataFrames and Datasets are used in modern spark applications.\nIn its core, Spark has three main components."
  },
  {
    "objectID": "posts/2026-01-14-spark-architecture.html#driver-program",
    "href": "posts/2026-01-14-spark-architecture.html#driver-program",
    "title": "Essentials of Spark Architecture",
    "section": "Driver Program:",
    "text": "Driver Program:\nAny script that uses spark to process big (and not so big) data is a spark application. The Driver program is the entry point of the application and it is where the main script is hosted.\nThe driver program reads the script and creates a logical plan of operations Directed Acyclic Graph. In spark, there are two types of operations. Transformations and actions. The DAG is comprised of these transformations and actions. The smallest sub unit of any operation is a task and a set of tasks are grouped in a stage. Spark DAG is built according to the logic of the user’s application. The logical DAG is broken down into stages and TaskScheduler serializes and assigns individual tasks. The Driver program stores the metadata of the tasks shared with the executors and the results are collected back once the tasks are completed. The taks lineage is readily available in the driver so that it can quickly rebuild a lost partition in case a failure. The process of building serialized stages is called DAG scheduling. The scheduled DAG stages are then transferred to Task Scheduler. TaskScheduler assigns tasks to executors. Actual execution happens only when an action is required, this is the so called ‘Lazy Evaluation’."
  },
  {
    "objectID": "posts/2026-01-14-spark-architecture.html#sparksession-and-sparkcontext",
    "href": "posts/2026-01-14-spark-architecture.html#sparksession-and-sparkcontext",
    "title": "Essentials of Spark Architecture",
    "section": "SparkSession and SparkContext",
    "text": "SparkSession and SparkContext\n\nSparkSession\nSparkSession is the latest implementation in Spark v&gt;2.x.x. The Driver program creates SparkSession which is the entry point of a program. In SparkSession, along with SparkContext, SQL, Streaming, ML and Graph context are available and unified into one engine under the same SparkSession offering superior flexibility. SparkContext is now available as backward compatible to access low level RDD features. Since the higher level APIs are accessible with SparkSession, Apache recommends to initialize with SparkSession.\n\n\nSparkContext\nSparkContext is created by the driver program and this was formerly the entry point until spark 2.x.x. SparkContext connects to the cluster manager and acquires the required resources for the distributed computation. It then sends the serialized DAG stages to the executor nodes and keeps track of the execution via regular heart beat messages.\n\n\nCluster manager :\nThe computing resources are managed by the cluster manager. There are three different types of cluster management in Spark. Namely, Spark standalone, Kubernetes and YARN. Often the resource manager resides in master node but this depends on the deployment mode of the application. In local deployment mode, The driver and executor nodes are hosted in a single thread and spark manages the threads itself and no cluster manager is involved in this case.\nThe resource manager receives requests from the task scheduler and launches executor processes on the worker nodes. The executor process(es) are launched in pods (kubernetes), containers /raw processes (spark standalone) depending on the configuration of the deployment. It then pings back the availability of the worker nodes and a connection is established between the spark driver program and the worker nodes.\n\n\nExecutors\nThe executor manages the task execution and manages the memory, it communicates with the driver program to report the status of the assigned tasks and return the computed results. The executors are also involved in shuffle operations and act as a server for other executors to access the local data of the executor."
  },
  {
    "objectID": "posts/2026-01-26-spark-query-optimization.html",
    "href": "posts/2026-01-26-spark-query-optimization.html",
    "title": "Essentials of Spark Architecture",
    "section": "",
    "text": "Apache Spark has a query optimization engine that rewrites the user code for optimum computational cost and execution efficiency. The optimization process start is carried out in several steps from parsing the code to executing the optimized generated code.\n\n\n\nSpark Query Optimization Steps\n\n\n\nParsing\nEach SQL / Hive queries are first read in the Parsing process. This parsing process converts the input into Abstract Syntax Tree (AST). This AST is called Unresolved Logical Plan.\n\n\nAnalysis\nThe Unresolved Logical Plan is validated for Table/Column names and data types by looking up into the catalog. It also resolves the attributes and binds to the actual schema. It also verifies the functions that is actually available. After Analyzer Rules Execution, this steps yields an entity known as Resolved Logical Plan\n\n\nLogical Optimization\nThe Resolved Logical Plan is then undergoes into a Rule-Based Optimization step. There are several rules that are applied here and some of the key optimization rules are\n\nPredicate Pushdown: This rule prepones the filter operation as early as possible in the query execution plan. The idea is to reduce the volume of data that flows in the operation as early as possible.\nCombine Filters: This rule combines similar filter operations into one.\nProjection Pruning: This rule selects only the required columns that are implicated in the query.\nNull Propagation: Optimize null checks\nConstant Propagation: if there is any computation involves using a constant, it is simplified at the beginning and not executed repeatedly through out the query execution.\n\nThe logical optimization steps yields Optimized Logical Plan\n\n\nPhysical Planning\nThe Optimized Logical Plan is then evaluated against a cost model for several alternative plans. The estimation of cost is projected based on\n\nData size\nComplexity of the operation\nNetwork shuffle cost\nResource requirements\n\nThe least expensive model is selected in this step and it is called Selected Physical Plan.\n\n\nCode Generation and execution\nThe selected physical plan is then taken as a base and an optimized code and the execution is then carried out as described in Essentials of Spark Architecture.\n\n\nVisualizing the Spark Optimization Steps\nThe selected physical plans can be read using df.explain() | df.explain(\"simple\") and all the steps of the optimization plan can be seen df.explain(\"extended\"). The generated code can be seen using df.explain(\"codegen\") and df.explain(\"cost\") can print the logical plan and statistics if they are available."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Complete archive of my writings on technology, data, and distributed systems."
  },
  {
    "objectID": "posts/index.html#all-posts",
    "href": "posts/index.html#all-posts",
    "title": "Blog",
    "section": "",
    "text": "Complete archive of my writings on technology, data, and distributed systems."
  },
  {
    "objectID": "posts/index.html#latest-posts",
    "href": "posts/index.html#latest-posts",
    "title": "Blog",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n\n\n\n\n\n\n\n\nEssentials of Spark Architecture\n\n\n\nSpark\n\ndata-engineering\n\ndistributed-computing\n\npyspark\n\nsparkSQL\n\nspark-optimization\n\n\n\nEssentials of Spark Query Optimization Process.\n\n\n\n\n\nJan 26, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nEssentials of Spark Architecture\n\n\n\nSpark\n\ndata-engineering\n\ndistributed-computing\n\npyspark\n\nsparkSQL\n\n\n\nEssentials of how Spark is constructed and inner mechanics of how a Spark job is executed.\n\n\n\n\n\nJan 14, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nEssentials of Apache Spark\n\n\n\ndistributed-computing\n\ndata-engineering\n\napache-spark\n\n\n\nA brief summary of Apache Spark and its core features.\n\n\n\n\n\nJan 5, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nSpark and Big data Engineering\n\n\n\ndistributed-computing\n\ndata-engineering\n\napache-spark\n\n\n\nEssentials of Big Data Engineering. In essence, all big data processing is distributed computing.\n\n\n\n\n\nDec 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeaving Code: When the Loom Met the Algorithm\n\n\n\nHistory of Computing\n\nPersonal Reflection\n\n\n\nA reflection on how punched cards from 1801 evolved into modern AI, completing a perfect technological circle.\n\n\n\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Data Pipeline is a distributed system. Start Architecting It Like One\n\n\n\ndocker\n\ndata-engineering\n\ndistributed-computing\n\npython\n\n\n\nA Data pipeline is indeed a distributed system since it usually has more than one component. It must be orchestrated properly to be able to work everywhere. This article demonstrates a simple ETL pipeline that does not break when deployed in production.\n\n\n\n\n\nNov 8, 2025\n\n\n\n\n\nNo matching items\n\n← Back to Home"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Personal Website",
    "section": "",
    "text": "I am Balasubramaniam Namasivayam, a Data Engineer passionate about building scalable data pipelines and distributed systems.\nThis site serves as my personal portfolio and blog where I share my learnings and projects.\n\n\n\n\nExplore my projects\n\nExplore →\n\n\n\n\nBrowse my latest articles\n\nExplore →"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Welcome to My Personal Website",
    "section": "",
    "text": "I am Balasubramaniam Namasivayam, a Data Engineer passionate about building scalable data pipelines and distributed systems.\nThis site serves as my personal portfolio and blog where I share my learnings and projects.\n\n\n\n\nExplore my projects\n\nExplore →\n\n\n\n\nBrowse my latest articles\n\nExplore →"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "This section is being updated. In the meantime, please feel free to checkout my github. Thank you for your visit."
  },
  {
    "objectID": "posts/2025-12-23-Bigdata-processing.html",
    "href": "posts/2025-12-23-Bigdata-processing.html",
    "title": "Spark and Big data Engineering",
    "section": "",
    "text": "Large-scale data processing is almost always accomplished using distributed computing. This remains true in the verge of 2026, while we wait for IBM to deliver the next big quantum computing breakthrough.\nBroadly speaking, there are two principal approaches to distributed data processing. As Peter Griffin would say, hitting two birds with one stone requires either two small birds or one very big stone. Distributed systems follow the same logic: either by massively scaling the computing infrastructure to process large datasets all at once, or break the data into smaller chunks (so-called “partitions”) that can be processed efficiently on ordinary machines with moderate resources.\n\n\n\nBig Data Engineering is Distributed Computing\n\n\nBuilding high-performance computing clusters (HPC) requires significant investment, specialized resource allocation managers (like SLURM) and physical space. In contrast, software frameworks such as the Hadoop Distributed File System (HDFS), MapReduce enabled creating partitions and achieving parallelism across commodity hardware, effectively democratizing large-scale data processing. Apache Spark (written in Scala) went further by leveraging in-memory computing, an inbuilt cluster manager, along with APIs for popular programming languages such as Python, R and Java. This eliminated the performance bottlenecks that persisted even with Hadoop. In a short period, Apache Spark became the go-to framework for big data engineering across diverse domains."
  },
  {
    "objectID": "posts/2025-12-23-Bigdata-processing.html#big-data-processing",
    "href": "posts/2025-12-23-Bigdata-processing.html#big-data-processing",
    "title": "Spark and Big data Engineering",
    "section": "",
    "text": "Large-scale data processing is almost always accomplished using distributed computing. This remains true in the verge of 2026, while we wait for IBM to deliver the next big quantum computing breakthrough.\nBroadly speaking, there are two principal approaches to distributed data processing. As Peter Griffin would say, hitting two birds with one stone requires either two small birds or one very big stone. Distributed systems follow the same logic: either by massively scaling the computing infrastructure to process large datasets all at once, or break the data into smaller chunks (so-called “partitions”) that can be processed efficiently on ordinary machines with moderate resources.\n\n\n\nBig Data Engineering is Distributed Computing\n\n\nBuilding high-performance computing clusters (HPC) requires significant investment, specialized resource allocation managers (like SLURM) and physical space. In contrast, software frameworks such as the Hadoop Distributed File System (HDFS), MapReduce enabled creating partitions and achieving parallelism across commodity hardware, effectively democratizing large-scale data processing. Apache Spark (written in Scala) went further by leveraging in-memory computing, an inbuilt cluster manager, along with APIs for popular programming languages such as Python, R and Java. This eliminated the performance bottlenecks that persisted even with Hadoop. In a short period, Apache Spark became the go-to framework for big data engineering across diverse domains."
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html",
    "href": "posts/2025-11-09-docker-data-pipeline.html",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "",
    "text": "A data pipeline already a distributed system since it has multiple component services. Treating it anything less is why pipelines break when moved from a local machine to production. To build a system that works everywhere, one must consider service discovery, configuration management and network isolation right from the beginning.\nLet us a build production ready and automated data ingestion pipeline using docker. We will first build a simple python app that takes a csv and stores in a PostgreSQL database, then evolves into a multi-container architecture where a data-importer service automatically ingests data into a dedicated DBMS.\nFor this data pipeline, lets get Paris AirBnB Listings data and build that data importer app. This app will take this csv data and store it into a database inside the PostgreSQL container. To make this app usable for any data, we will parametrize all the input variables.\nThis app takes a CSV file and creates a URL object with parameterized connection details. In isolated environments, Docker’s DNS system resolves service names to container IPs automatically. Everything is parameterized for reuse."
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html#containerizing-the-application",
    "href": "posts/2025-11-09-docker-data-pipeline.html#containerizing-the-application",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "Containerizing the Application",
    "text": "Containerizing the Application\nWe can containerize this app simply using a Dockerfile. Assuming this script is stored under ./src/data_importer.py and the downloaded data is placed in data/01_paris_airbnb_listings.csv. We will need pandas, sqlalchemy and psycopg2 as dependencies.\nLet’s create a requirements.txt in the current working directory that will be used to build the docker image.\npandas==2.3.1\nnumpy==2.0.2\nSQLAlchemy==2.0.43\npsycopg2-binary==2.9.11\nWe will use python 3.9-slim as a base image and install the dependencies copy the data and script. We will run the app inside the container and the arguments variables can be passed using the CMD operator.\nFROM python:3.9-slim\nWORKDIR /data_importer\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY ./data ./src ./\nENTRYPOINT [\"python\",\"data_importer.py\"]\nCMD [\"01_paris_airbnb_listings.csv\",\"sqluser\",\"secret\",\"dbms\",\"5432\", \"global_data\",\"paris_airbnb\" ]\nThe image can be built using.\ndocker build -t data_importer .\nThe above command will yield an image named data_importer."
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html#orchestrating-with-docker-compose",
    "href": "posts/2025-11-09-docker-data-pipeline.html#orchestrating-with-docker-compose",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "Orchestrating with Docker Compose",
    "text": "Orchestrating with Docker Compose\nOnce the image is successfully built, we can create a distributed system using docker-compose. We will use PostgreSQL 14 image for database and create a database service dbms. We will use a named volume pgdata for data persistence and use a mounted volume to enable communication between the data_importer container. The mounted volume will be mapped between the working directory of the data_importer app and the physical location in the local machine. This way, the system can take any data inside the local data directory and import to the SQL database automatically and securely. We will use a bridge network named telnet to enable container-to -container communication. lets save these parameters in a docker-compose.yml in the current working directory.\nservices :\n  dbms :\n    image : postgres:14\n    environment:\n      POSTGRES_DB: global_data\n      POSTGRES_USER: sqluser\n      POSTGRES_PASSWORD: secret\n    volumes :\n      - pgdata:/var/lib/postgresql/data\n      - ./data:/imported_data\n    ports :\n      - \"5432:5432\"\n    networks :\n      - telnet\n  \n  dataimporter:\n    image: data_importer\n    depends_on:\n      - dbms\n    networks:\n      - telnet\n    command: [\"01_paris_airbnb_listings.csv\",\"sqluser\",\"secret\",\"dbms\",\"5432\",\"global_data\",\"paris_airbnb_data\"]\n  \nvolumes :\n  pgdata :\n\nnetworks :\n  telnet :\n    driver : bridge"
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html#deployment",
    "href": "posts/2025-11-09-docker-data-pipeline.html#deployment",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "Deployment",
    "text": "Deployment\nThe complete ETL pipeline can be deployed using:\ndocker-compose up -d\nUpon successful execution, the data would be safely stored inside the global_data database in the dbms server. This can be verified using a docker exec command.\ndocker exec -it sql_dbms_1 psql -U sqluser -d global_data\nWhich yields\n\npsql (14.19 (Debian 14.19-1.pgdg13+1))  \nType \"help\" for help.  \n  \nglobal_data=# \\dt  \n             List of relations  \nSchema |       Name        | Type  |  Owner     \n--------+-------------------+-------+---------  \npublic | paris_airbnb_data | table | sqluser  \n(1 row)  \n  \nglobal_data=#\nNote that the table_name is correctly named as paris_airbnb_data contary to paris_airbnb as mentioned in the Dockerfile. This suggests that the data is correctly passed from the argument variables defined in the command operator in the docker-compose.yml."
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html#summary",
    "href": "posts/2025-11-09-docker-data-pipeline.html#summary",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "Summary",
    "text": "Summary\nIn this guide, we transformed a simple Python app into a production-ready data pipeline by adopting distributed systems principles.\n\nService Discovery: Containers communicate through Docker DNS (dbms) instead of hard-coded IPs.\nConfiguration Management: All parameters are externalized through CLI arguments and environment variables.\nNetwork Isolation: Secure bridge network enables container-to-container communication.\nData Persistence: Named volumes ensure database survival across container restarts.\nOrchestration: Docker Compose manages multi-service dependencies and lifecycle.\n\nTreating data pipelines as a distributed systems from the beginning eliminates the system breakdown across environments and creates infrastructure that scales from local development to production deployment.\nThe complete pipeline is available in GitHub."
  },
  {
    "objectID": "posts/2026-01-05-spark.html",
    "href": "posts/2026-01-05-spark.html",
    "title": "Essentials of Apache Spark",
    "section": "",
    "text": "Traditionally, processing big data was challenging because the sheer size of the data that exceeds far greater than the available compute resources. To handle such big data, Hadoop had “Hadoop Distributed File System (HDFS)” framework and batch processing through the MapReduce function. These innovations enabled distributed computing to run on commodity hardware. While this offered many advantages such as partitioned data, distributed processing etc, significant challenges remained in terms of scalability and speed since it is based on disc. This is because this way of distributed processing resulted in exponential read write cycles for each intermediate steps.\nSpark was developed to address these important bottlenecks in terms of speed and efficiency by adopting “in-memory computing”. This drasticially improved speed and efficiency of big data processing. There were many additional advantages in Spark which is detailed below."
  },
  {
    "objectID": "posts/2026-01-05-spark.html#apache-spark",
    "href": "posts/2026-01-05-spark.html#apache-spark",
    "title": "Essentials of Apache Spark",
    "section": "Apache Spark",
    "text": "Apache Spark\nApache Spark was developed by Matei Zaharia of University of California, Berkeley in 2009 and was donated to Apache Software Foundation in 2013.\nSpark is a distributed computing framework that addressed bottlenecks in the traditional Hadoop based big data tools. Spark achieves performance improvement on the following fronts:\n\nIn-memory computing\nPartitioning\nFault-tolerant computing\nUnified engine for big data ecosystem\n\n\n\n\nApache Spark\n\n\n\nIn-memory computing:\nSpark works via in-memory computing that retains the data cached in the memory. This alone can yield 100-1000x speedups. Imagine a computation with many steps. At each step, the data must be read from the disk, loaded into memory (RAM), processed, intermediate results are written back. This operation must be repeated until the operation is completed. Clearly, the bottleneck is the repeated read, write cycle of intermediary data to hard disk. Spark eliminates most of this repetitive I/O on the disk by keeping the intermediate results in memory, dramatically accelerating performance. Spark intelligently manages the memory by keeping maximum of the partition data in memory (RAM). When the partition data exceeds the allocated in-memory, spark spills over the least frequently used partitions to the physical memory (disk) and quickly read back if and when the partition is needed.\n\n\nAbstractions in Spark\nSpark developed its core abstraction called Resilient Distributed Datasets (RDD) in the 2010s followed by the now popular Dataset and DataFrame which are the latest (introduced 2015-2016) higher level API implementations of Spark. DataFrames and Datasets are the preferred APIs today for their optimizations and ease of use. The RDDs still remain functional and used for specialized use cases.\n\n\nData Partitioning:\nSpark partitions the data either by a variable in the data or arbitrarily by equal size. Partitioning by a variable of the data frame can be created using partitionby() method and arbitrary partitions can be created using repartition() method. Proper partitioning is critical for parallelism and optimized performance.\n\n\nFault-tolerant computing:\nSpark is fault tolerant since it can rapidly recreate the partition data on any failed nodes. Sparks tracks the lineage of the partition from the original data. If a node fails, spark rebuilds the lost data without ever stopping or restarting the process. This enabled horizontal scaling to handle petabytes of data reliably.\n\n\nSpark has an Unified Engine\nTraditional Hadoop based solutions relied on different independent frameworks for specific processing of the data. For example for bigdata processing, there was MapReduce, for Stream Processing there was Strom or Samza, for SQL queries there was Hive or Impala. These resulted in a data silos, complex architectures and expensive data overheads. This is because the engineers had to master many tools and integrate into their system. Spark has the core engine and has several higher level APIs such as Spark SQL, Structured Streaming, Spark MLlib and Spark Graphx for querying, real-time data processing, machine learning and graph analytics all unified in the Spark framework, making it a powerful modern data stack.\n\n\nSpark is Polyglot\nSpark’s core is written in Scala and APIs for Python (pyspark), Java, R (sparklyr) and SQL are available making it a powerful language across different data and statistical stacks. Spark documentation describes near identical performance across these languages.\n\n\nConclusion\nApache Spark revolutionized big data processing by providing a unified, in-memory computing framework that addressed the limitations of traditional Hadoop ecosystems. By combining in-memory processing, intelligent partitioning, fault tolerance, and a polyglot API approach, Spark delivers dramatic performance improvements while simplifying complex data architectures. Its ability to handle diverse workloads from batch processing and real-time streaming to machine learning and graph analytics makes it a powerful engine for today’s big data challanges. All these operations are efficiently performed through a single integrated engine makes it a cornerstone of modern data platforms. As data volumes continue to grow and processing demands become more complex, Spark’s balanced approach of performance, reliability, and developer accessibility ensures its enduring relevance in the data engineering landscape."
  },
  {
    "objectID": "posts/2025-11-27-jacquard.html",
    "href": "posts/2025-11-27-jacquard.html",
    "title": "Weaving Code: When the Loom Met the Algorithm",
    "section": "",
    "text": "Do you know that the most famous image in the early history of computing is actually a piece of fabric? It is a portrait of a French engineer who lived during the Napoleonic Era, woven entirely on silk.\n\n\n\nThe Most Famous Image in the Early History of Computing. This is actually a portrait of a silk fabric on which the image of Jacquard was woven. Portrait by Michel Marie Carquillat after Claude Bonnefond (circa 1839). Public domain via Wikimedia Commons.\n\n\nJoseph Marie Jacquard was born in Lyon, France and was a weaver and master inventor. He was celebrated by the Emperor Napoleon for the invention of a “programmable loom”. Jacquard’s idea of using punched cards to represent where a thread must go up or down in fine silk fabric to produce motifs and patterns with unlimited resolution immediately became a royal sensation.\nCharles Babbage had not only personally possessed one of these portraits of Jacquard woven in silk using 24,000 punched cards, but was also deeply inspired by it. He adopted these very punched cards as the input device for his Analytical Engine.\nAfter all, transcribing a complex design onto fabric by controlling threads with punched cards is a physical demonstration of binary logic, waiting to be spotted by a visionary like Babbage. Later, IBM produced computers that used punched cards and card readers. Early computer programs were written on paper, translated into punched cards, then loaded into a computer to be read and executed.\nWait - why am I boring you with the history of computers?\nA couple of days ago, I saw a post in my feed where someone uploaded a photo to an LLM and asked it to return a pattern so his wife could knit the photo into socks or a handbag.\nIt was in that moment I saw the circle close. The punched cards of the 1800s have now evolved into artificial neural networks, only to give back a pattern showing where a thread must go up or down in a fabric.\n\n\n\nAI-generated representation of Joseph Marie Jacquard with a punched card\n\n\nI started my career as a textile engineer, and I still vividly remember creating punches in cardboard for the Jacquard loom using a piano card-punching machine during practical sessions. Fast forward to this day of writing, I am now living in Lyon and working at the intersection of informatics and complex data, still looking for patterns not on fabric but on data.\nI just had a moment of awe witnessing the threads of technology woven into a perfect circle across the fabric of space and time. This convergence felt deeply personal. It is a powerful reminder that technology often spirals upward instead of progressing linearly.\n\n\n\nAI-generated visual summary of this blog post. Thanks to NotebookLM de Google"
  }
]