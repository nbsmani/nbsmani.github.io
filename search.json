[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "As a highly motivated Data Engineer, I am dedicated to driving value and innovation within the organizations I collaborate with. Senior Data Engineer with a PhD in Engineering Sciences and 8+ years of experience designing and deploying scalable, containerized data pipelines into production. Expert in Python, SQL, PySpark, and modern orchestration (Airflow) and infrastructure (Docker, AWS) tools. Proven ability to architect full data lifecycle solutions from distributed processing on HPC/Spark clusters to workflow automation and cloud storage ensuring data integrity, reproducibility, and performance. Seeking to apply deep systems- engineering expertise to data infrastructure challenges in the tech industry. I am committed to delivering high-performance outcomes while pursuing personal and professional growth in a dynamic, results-driven environment.\n\nBig Data Technologies: Spark, Kafka\nCloud Platforms: AWS, GCP, Azure\nContainerization: Docker, Kubernetes\nLanguages: Python, SQL,R\n\nI enjoy solving complex data problems and optimizing performance for large-scale applications."
  },
  {
    "objectID": "posts/2025-11-27-jacquard.html",
    "href": "posts/2025-11-27-jacquard.html",
    "title": "Weaving Code: When the Loom Met the Algorithm",
    "section": "",
    "text": "Do you know that the most famous image in the early history of computing is actually a piece of fabric? It is a portrait of a French engineer who lived during the Napoleonic Era, woven entirely on silk.\n\n\n\nThe Most Famous Image in the Early History of Computing. This is actually a portrait of a silk fabric on which the image of Jacquard was woven. Portrait by Michel Marie Carquillat after Claude Bonnefond (circa 1839). Public domain via Wikimedia Commons.\n\n\nJoseph Marie Jacquard was born in Lyon, France and was a weaver and master inventor. He was celebrated by the Emperor Napoleon for the invention of a “programmable loom”. Jacquard’s idea of using punched cards to represent where a thread must go up or down in fine silk fabric to produce motifs and patterns with unlimited resolution immediately became a royal sensation.\nCharles Babbage had not only personally possessed one of these portraits of Jacquard woven in silk using 24,000 punched cards, but was also deeply inspired by it. He adopted these very punched cards as the input device for his Analytical Engine.\nAfter all, transcribing a complex design onto fabric by controlling threads with punched cards is a physical demonstration of binary logic, waiting to be spotted by a visionary like Babbage. Later, IBM produced computers that used punched cards and card readers. Early computer programs were written on paper, translated into punched cards, then loaded into a computer to be read and executed.\nWait - why am I boring you with the history of computers?\nA couple of days ago, I saw a post in my feed where someone uploaded a photo to an LLM and asked it to return a pattern so his wife could knit the photo into socks or a handbag.\nIt was in that moment I saw the circle close. The punched cards of the 1800s have now evolved into artificial neural networks, only to give back a pattern showing where a thread must go up or down in a fabric.\n\n\n\nAI-generated representation of Joseph Marie Jacquard with a punched card\n\n\nI started my career as a textile engineer, and I still vividly remember creating punches in cardboard for the Jacquard loom using a piano card-punching machine during practical sessions. Fast forward to this day of writing, I am now living in Lyon and working at the intersection of informatics and complex data, still looking for patterns not on fabric but on data.\nI just had a moment of awe witnessing the threads of technology woven into a perfect circle across the fabric of space and time. This convergence felt deeply personal. It is a powerful reminder that technology often spirals upward instead of progressing linearly.\n\n\n\nAI-generated visual summary of this blog post. Thanks to NotebookLM de Google"
  },
  {
    "objectID": "posts/2026-02-03-docker-dns-resolution.html",
    "href": "posts/2026-02-03-docker-dns-resolution.html",
    "title": "A guide to resolving DNS issues in Docker containers when accessing external APIs.",
    "section": "",
    "text": "When developing containerized applications, sometimes the application need to fetch data from internet.Let’s say developing an ETL pipline that tracks the price of gold, (which is currently very poplular :)). There are public API endpoints available and one should be able to pull the data in a python script. For example, one can use the following script to pull data from a public API (luckily this one is free and not rate-limited).\nThis script works perfectly in a local machine and saves the clean data into the destination of the pipeline. Great!\nBut challenge arises when you want to containerize your application. Yes using docker-compose.yml, one can create a bridge network and you can orchestrate the communication between containers. I have previously written about it here . But you might want to verify the image before you orchestrate and when you build the app image using Dockerfile, with the command docker build -t &lt;your-img-tag&gt; ., you will get the image be built without any problem. But when you run the app locally to verify it works as intended, you will get ConnectionError because the container is isolated from your local network.\nSo how does one solve this problem?\nThis is where --dns flag comes into play. you can simply instruct the image to resolve the ipaddress to a public server (such as google’s dns service) That single flag, --dns 8.8.8.8, instructs the container to use Google’s public DNS server for resolving hostnames."
  },
  {
    "objectID": "posts/2026-02-03-docker-dns-resolution.html#why-does-this-happen",
    "href": "posts/2026-02-03-docker-dns-resolution.html#why-does-this-happen",
    "title": "A guide to resolving DNS issues in Docker containers when accessing external APIs.",
    "section": "Why Does This Happen?",
    "text": "Why Does This Happen?\n\nThe Two-Phase Network Problem\n\nBuild Phase: During docker build, Docker uses your host’s DNS configuration. That’s why pip install (which needs to resolve pypi.org) works fine.\nRun Phase: When docker run executes, the container starts with Docker’s default networking setup, which may not include functional DNS servers. Your script tries to resolve api.gold-api.com, fails, and throws a ConnectionError.\n\n\n\nUnderstanding Docker’s Default DNS Behavior\nBy default, Docker containers use the host’s DNS settings. However, in some configurations (especially on Linux without systemd-resolved or with certain network managers), this inheritance breaks down. The container ends up with an empty /etc/resolv.conf or one pointing to a non-existent DNS server."
  },
  {
    "objectID": "posts/2026-01-26-spark-query-optimization.html",
    "href": "posts/2026-01-26-spark-query-optimization.html",
    "title": "Query Optimization in Apache Spark: A Deep Dive",
    "section": "",
    "text": "Apache Spark has a query optimization engine that rewrites the user code for optimum computational cost and execution efficiency. The optimization process start is carried out in several steps from parsing the code to executing the optimized generated code.\n\n\n\nSpark Query Optimization Steps\n\n\n\nParsing\nEach SQL / Hive queries are first read in the Parsing process. This parsing process converts the input into Abstract Syntax Tree (AST). This AST is called Unresolved Logical Plan.\n\n\nAnalysis\nThe Unresolved Logical Plan is validated for Table/Column names and data types by looking up into the catalog. It also resolves the attributes and binds to the actual schema. It also verifies the functions that is actually available. After Analyzer Rules Execution, this steps yields an entity known as Resolved Logical Plan\n\n\nLogical Optimization\nThe Resolved Logical Plan is then undergoes into a Rule-Based Optimization step. There are several rules that are applied here and some of the key optimization rules are\n\nPredicate Pushdown: This rule prepones the filter operation as early as possible in the query execution plan. The idea is to reduce the volume of data that flows in the operation as early as possible.\nCombine Filters: This rule combines similar filter operations into one.\nProjection Pruning: This rule selects only the required columns that are implicated in the query.\nNull Propagation: Optimize null checks\nConstant Propagation: if there is any computation involves using a constant, it is simplified at the beginning and not executed repeatedly through out the query execution.\n\nThe logical optimization steps yields Optimized Logical Plan\n\n\nPhysical Planning\nThe Optimized Logical Plan is then evaluated against a cost model for several alternative plans. The estimation of cost is projected based on\n\nData size\nComplexity of the operation\nNetwork shuffle cost\nResource requirements\n\nThe least expensive model is selected in this step and it is called Selected Physical Plan.\n\n\nCode Generation and execution\nThe selected physical plan is then taken as a base and an optimized code and the execution is then carried out as described in Essentials of Spark Architecture.\n\n\nVisualizing the Spark Optimization Steps\nThe selected physical plans can be read using df.explain() | df.explain(\"simple\") and all the steps of the optimization plan can be seen df.explain(\"extended\"). The generated code can be seen using df.explain(\"codegen\") and df.explain(\"cost\") can print the logical plan and statistics if they are available."
  },
  {
    "objectID": "posts/2026-02-22-spark-dataframes.html",
    "href": "posts/2026-02-22-spark-dataframes.html",
    "title": "DataFrames in Spark: A Comprehensive Guide",
    "section": "",
    "text": "Dataframes in Spark are structured and labelled data in two dimensions like a spreadsheet or table. Spark dataframes are built over Resilient Distributed Dataset (RDD) and therefore stored in several partitions.\nSpark Documentation describes Dataframe as\n\n“A DataFrame is a DataSet organized into named columns. It is conceptually equivalent to a table in a relational database or a dataframe in R/Python, but with richer optimizations under the hood.”"
  },
  {
    "objectID": "posts/2026-02-22-spark-dataframes.html#what-is-a-spark-dataframe",
    "href": "posts/2026-02-22-spark-dataframes.html#what-is-a-spark-dataframe",
    "title": "DataFrames in Spark: A Comprehensive Guide",
    "section": "",
    "text": "Dataframes in Spark are structured and labelled data in two dimensions like a spreadsheet or table. Spark dataframes are built over Resilient Distributed Dataset (RDD) and therefore stored in several partitions.\nSpark Documentation describes Dataframe as\n\n“A DataFrame is a DataSet organized into named columns. It is conceptually equivalent to a table in a relational database or a dataframe in R/Python, but with richer optimizations under the hood.”"
  },
  {
    "objectID": "posts/2026-02-22-spark-dataframes.html#features-of-dataframe",
    "href": "posts/2026-02-22-spark-dataframes.html#features-of-dataframe",
    "title": "DataFrames in Spark: A Comprehensive Guide",
    "section": "Features of DataFrame",
    "text": "Features of DataFrame\nOver traditional dataframes, Spark DataFrame offers the following features.\n\nImmutable: Spark dataframes are immutable and any transformation made on the dataframe result in creation of new dataframe.\nLazy Evaluation: Spark dataframes are lazily evaluated. When a transformation is applied over a dataframe, an optimized execution plan is created and the execution is carried out only when an action is called.\nOptimized: Spark dataframe is a high level API of RDD and the executions are optimized (logical and physical). The detailed optimization can be read in this article .\nFault Tolerant: Since the spark DataFrame are built over the RDD structure, they are inherently fault tolerant. When a node or partition fails, the replacement partition is recreated by tracking the lineage without restarting the process from scratch or stopping the process.\nSchema Aware: Spark dataframes hold defined datatypes and constraints for each column of the dataframe. It offers schema flexibility by supporting schema evolution and dynamic typing.\n\nOne notable difference between a pandas dataframe and a spark DataFrame is that pd.DataFrame is index aware where as the records in the spark DataFrame are not indexed but stored as partitions, this is known as partition aware property.\n\n\n\nAI summarized infographic of the features and operations of Spark DataFrames"
  },
  {
    "objectID": "posts/2026-02-22-spark-dataframes.html#creating-a-dataframe",
    "href": "posts/2026-02-22-spark-dataframes.html#creating-a-dataframe",
    "title": "DataFrames in Spark: A Comprehensive Guide",
    "section": "Creating a Dataframe",
    "text": "Creating a Dataframe\nA dataframe can be created by the following 3 ways:\n\nFrom an external file\nSpark offers several file formats such as csv, json, parquet, ORC, avro, hdfs.. Dataframe can be created from any of these sources using spark.read.csv(\"file_path\", args) or spark.read.format(\"format_name\").load(\"file_path\") which is consistent for all the file formats.\n\n\nFrom another df like object\nSpark dataframes can also be created using traditional data containers such as list of dictionaries / tuples or from pandas DataFrame object. This can be done using spark.createDataFrame(df_name)\nfrom pyspark.sql import SparkSession\n#create a SparkSession\n\nspark = SparkSession.builder.appName(\"EmployeeDataFrame\").getOrCreate()\n\nemployees = [\n  {\"name\": \"John D.\", \"age\": 30, \"department\": \"HR\"},\n  {\"name\": \"Alice G.\", \"age\": 25, \"department\": \"Finance\"},\n  {\"name\": \"Bob T.\", \"age\": 35, \"department\": \"IT\"},\n  {\"name\": \"Eve A.\", \"age\": 28, \"department\": \"Marketing\"}\n]\ndf = spark.createDataFrame(employees)\n#Select only the name and age columns\nnew_df = df.select(\"name\", \"age\")\n\n#Since DataFrame is lazily evaluated, we need to call an explicit action to see the results.\n\nnew_df.show()\n+-------+---+\n|   name|age|\n+------- +---+\n|John D. | 30|\n|Alice G.| 25|\n|  Bob T.| 35|\n|  Eve A.| 28|\n+-------+---+\n\n\nFrom an existing table in the SparkSession\nIf there is a named table available in the SparkSession, it can be read as a DataFrame using read.table() method. For example, lets assume a sales table is available in the SparkSession\n#create a SparkSession\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n\ndf = spark.read.table(\"sales\")\ndf.show()\n\n#this will show the existing SQL table now stored as a Spark DataFrame\n+---------+----------+------+--------+--------------+\n| category|    price|product|quantity|  salesperson|\n+---------+----------+------+--------+--------------+\n|Electronics|   1200.5| Laptop|       5|   John Smith|\n|Electronics|    25.99|  Mouse|      20|  Alice Brown|\n| Furniture|    350.0|   Desk|       3|  Bob Johnson|\n| Furniture|    175.5|  Chair|       8|    Eve Davis|\n|Stationery|     4.99|Notebook|     100|   John Smith|\n|Stationery|     1.99|     Pen|     200|  Alice Brown|\n|Electronics|    450.0|Monitor|       4|  Bob Johnson|\n| Furniture|    280.0|Bookshelf|      2|    Eve Davis|\n+---------+----------+------+--------+--------------+\n\nAlternate methods to read a table\nThe table can also be read using a shorter table() orsql() function to store the output of a SQL query as DataFrame()\n##reading using `table()`\ndf = spark.table(\"sales\")\n\n## Storing the output of a SQL query as a Spark DataFrame\ndf = spark.sql(\"SELECT * FROM sales\" )"
  },
  {
    "objectID": "posts/2026-02-22-spark-dataframes.html#operations-on-spark-dataframes",
    "href": "posts/2026-02-22-spark-dataframes.html#operations-on-spark-dataframes",
    "title": "DataFrames in Spark: A Comprehensive Guide",
    "section": "Operations on Spark DataFrames",
    "text": "Operations on Spark DataFrames\nRDDs (thus Spark DataFrames) supports the following two types of operations\n\nTransformations\nActions\n\n\nTransformations\nTransformations create a new dataset from existing one In principle, the transformations are lazily evaluated. Once a transformation is called, a Directed Acyclic Graph (DAG) is built but it is not executed until an action is called. The following are some examples of transformations map(), filter(), sample(),distinct() union() join(), repartition()\n\n\nActions\nAction returns the evaluated result of the computation on the dataset to the driver program and to the user. The following are some examples of actions count(),first(),collect(), show()\n\nCommon Operations\nHere is an example of common operations on Spark DataFrame\n### Common DataFrame Operations\n\n# Once you have a DataFrame `df`, you can perform various transformations and actions:\n\n# Filtering\nfiltered_df = df.filter(df.age &gt; 28)\n\n# Adding columns\ndf_with_tax = df.withColumn(\"tax\", df.price * 0.1)\n\n# Aggregations\ndf.groupBy(\"department\").avg(\"age\").show()"
  },
  {
    "objectID": "posts/2026-02-22-spark-dataframes.html#conclusion",
    "href": "posts/2026-02-22-spark-dataframes.html#conclusion",
    "title": "DataFrames in Spark: A Comprehensive Guide",
    "section": "Conclusion",
    "text": "Conclusion\nSpark DataFrames are a powerful and flexible way to work with structured data in Apache Spark. They provide a high-level API for working with data, while also offering the performance and scalability benefits of Spark’s underlying RDD structure. By understanding how to create and manipulate DataFrames, one can unlock the full potential of Spark for the data processing needs."
  },
  {
    "objectID": "posts/2025-12-23-Bigdata-processing.html",
    "href": "posts/2025-12-23-Bigdata-processing.html",
    "title": "Spark and Big data Engineering",
    "section": "",
    "text": "Large-scale data processing is almost always accomplished using distributed computing. This remains true in the verge of 2026, while we wait for IBM to deliver the next big quantum computing breakthrough.\nBroadly speaking, there are two principal approaches to distributed data processing. As Peter Griffin would say, hitting two birds with one stone requires either two small birds or one very big stone. Distributed systems follow the same logic: either by massively scaling the computing infrastructure to process large datasets all at once, or break the data into smaller chunks (so-called “partitions”) that can be processed efficiently on ordinary machines with moderate resources.\n\n\n\nBig Data Engineering is Distributed Computing\n\n\nBuilding high-performance computing clusters (HPC) requires significant investment, specialized resource allocation managers (like SLURM) and physical space. In contrast, software frameworks such as the Hadoop Distributed File System (HDFS), MapReduce enabled creating partitions and achieving parallelism across commodity hardware, effectively democratizing large-scale data processing. Apache Spark (written in Scala) went further by leveraging in-memory computing, an inbuilt cluster manager, along with APIs for popular programming languages such as Python, R and Java. This eliminated the performance bottlenecks that persisted even with Hadoop. In a short period, Apache Spark became the go-to framework for big data engineering across diverse domains."
  },
  {
    "objectID": "posts/2025-12-23-Bigdata-processing.html#big-data-processing",
    "href": "posts/2025-12-23-Bigdata-processing.html#big-data-processing",
    "title": "Spark and Big data Engineering",
    "section": "",
    "text": "Large-scale data processing is almost always accomplished using distributed computing. This remains true in the verge of 2026, while we wait for IBM to deliver the next big quantum computing breakthrough.\nBroadly speaking, there are two principal approaches to distributed data processing. As Peter Griffin would say, hitting two birds with one stone requires either two small birds or one very big stone. Distributed systems follow the same logic: either by massively scaling the computing infrastructure to process large datasets all at once, or break the data into smaller chunks (so-called “partitions”) that can be processed efficiently on ordinary machines with moderate resources.\n\n\n\nBig Data Engineering is Distributed Computing\n\n\nBuilding high-performance computing clusters (HPC) requires significant investment, specialized resource allocation managers (like SLURM) and physical space. In contrast, software frameworks such as the Hadoop Distributed File System (HDFS), MapReduce enabled creating partitions and achieving parallelism across commodity hardware, effectively democratizing large-scale data processing. Apache Spark (written in Scala) went further by leveraging in-memory computing, an inbuilt cluster manager, along with APIs for popular programming languages such as Python, R and Java. This eliminated the performance bottlenecks that persisted even with Hadoop. In a short period, Apache Spark became the go-to framework for big data engineering across diverse domains."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "This section is being updated. In the meantime, please feel free to checkout my github. Thank you for your visit."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Personal Website",
    "section": "",
    "text": "I am Balasubramaniam Namasivayam, a Data Engineer passionate about building scalable data pipelines and distributed systems.\nThis site serves as my personal portfolio and blog where I share my learnings and projects.\n\n\n\n\nExplore my projects\n\nExplore →\n\n\n\n\nBrowse my latest articles\n\nExplore →"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Welcome to My Personal Website",
    "section": "",
    "text": "I am Balasubramaniam Namasivayam, a Data Engineer passionate about building scalable data pipelines and distributed systems.\nThis site serves as my personal portfolio and blog where I share my learnings and projects.\n\n\n\n\nExplore my projects\n\nExplore →\n\n\n\n\nBrowse my latest articles\n\nExplore →"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Complete archive of my writings on technology, data, and distributed systems."
  },
  {
    "objectID": "posts/index.html#all-posts",
    "href": "posts/index.html#all-posts",
    "title": "Blog",
    "section": "",
    "text": "Complete archive of my writings on technology, data, and distributed systems."
  },
  {
    "objectID": "posts/index.html#latest-posts",
    "href": "posts/index.html#latest-posts",
    "title": "Blog",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n\n\n\n\n\n\n\n\nDataFrames in Spark: A Comprehensive Guide\n\n\n\ndata engineering\n\nspark\n\ndataframes\n\n\n\nA comprehensive guide to understanding and working with DataFrames in Apache Spark, including their benefits, operations.\n\n\n\n\n\nFeb 22, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nData Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective\n\n\n\ndata engineering\n\n7s\n\nPersonal Reflection\n\n\n\nA reflection on how the Japanese concept of 7S used in factories could be related to modern Data warehousing principles.\n\n\n\n\n\nFeb 15, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nA guide to resolving DNS issues in Docker containers when accessing external APIs.\n\n\n\ndata engineering\n\ndocker\n\nnetworking\n\n\n\nDescription of DNS resolution issues in Docker and how to fix them using the –dns flag.\n\n\n\n\n\nFeb 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nQuery Optimization in Apache Spark: A Deep Dive\n\n\n\nSpark\n\ndata engineering\n\ndistributed computing\n\npyspark\n\nsparkSQL\n\nspark optimization\n\n\n\nEssentials of Spark Query Optimization Process.\n\n\n\n\n\nJan 26, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nEssentials of Spark Architecture\n\n\n\nSpark\n\ndata engineering\n\ndistributed computing\n\npyspark\n\nsparkSQL\n\n\n\nEssentials of how Spark is constructed and inner mechanics of how a Spark job is executed.\n\n\n\n\n\nJan 14, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nEssentials of Apache Spark\n\n\n\ndistributed computing\n\ndata engineering\n\napache spark\n\n\n\nA brief summary of Apache Spark and its core features.\n\n\n\n\n\nJan 5, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nSpark and Big data Engineering\n\n\n\ndistributed computing\n\ndata engineering\n\napache spark\n\n\n\nEssentials of Big Data Engineering. In essence, all big data processing is distributed computing.\n\n\n\n\n\nDec 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeaving Code: When the Loom Met the Algorithm\n\n\n\nHistory of Computing\n\nPersonal Reflection\n\n\n\nA reflection on how punched cards from 1801 evolved into modern AI, completing a perfect technological circle.\n\n\n\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Data Pipeline is a distributed system. Start Architecting It Like One\n\n\n\ndocker\n\ndata engineering\n\ndistributed computing\n\npython\n\n\n\nA Data pipeline is indeed a distributed system since it usually has more than one component. It must be orchestrated properly to be able to work everywhere. This article demonstrates a simple ETL pipeline that does not break when deployed in production.\n\n\n\n\n\nNov 8, 2025\n\n\n\n\n\nNo matching items\n\n← Back to Home"
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html",
    "href": "posts/2025-11-09-docker-data-pipeline.html",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "",
    "text": "A data pipeline already a distributed system since it has multiple component services. Treating it anything less is why pipelines break when moved from a local machine to production. To build a system that works everywhere, one must consider service discovery, configuration management and network isolation right from the beginning.\nLet us a build production ready and automated data ingestion pipeline using docker. We will first build a simple python app that takes a csv and stores in a PostgreSQL database, then evolves into a multi-container architecture where a data-importer service automatically ingests data into a dedicated DBMS.\nFor this data pipeline, lets get Paris AirBnB Listings data and build that data importer app. This app will take this csv data and store it into a database inside the PostgreSQL container. To make this app usable for any data, we will parametrize all the input variables.\nThis app takes a CSV file and creates a URL object with parameterized connection details. In isolated environments, Docker’s DNS system resolves service names to container IPs automatically. Everything is parameterized for reuse."
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html#containerizing-the-application",
    "href": "posts/2025-11-09-docker-data-pipeline.html#containerizing-the-application",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "Containerizing the Application",
    "text": "Containerizing the Application\nWe can containerize this app simply using a Dockerfile. Assuming this script is stored under ./src/data_importer.py and the downloaded data is placed in data/01_paris_airbnb_listings.csv. We will need pandas, sqlalchemy and psycopg2 as dependencies.\nLet’s create a requirements.txt in the current working directory that will be used to build the docker image.\npandas==2.3.1\nnumpy==2.0.2\nSQLAlchemy==2.0.43\npsycopg2-binary==2.9.11\nWe will use python 3.9-slim as a base image and install the dependencies copy the data and script. We will run the app inside the container and the arguments variables can be passed using the CMD operator.\nFROM python:3.9-slim\nWORKDIR /data_importer\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY ./data ./src ./\nENTRYPOINT [\"python\",\"data_importer.py\"]\nCMD [\"01_paris_airbnb_listings.csv\",\"sqluser\",\"secret\",\"dbms\",\"5432\", \"global_data\",\"paris_airbnb\" ]\nThe image can be built using.\ndocker build -t data_importer .\nThe above command will yield an image named data_importer."
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html#orchestrating-with-docker-compose",
    "href": "posts/2025-11-09-docker-data-pipeline.html#orchestrating-with-docker-compose",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "Orchestrating with Docker Compose",
    "text": "Orchestrating with Docker Compose\nOnce the image is successfully built, we can create a distributed system using docker-compose. We will use PostgreSQL 14 image for database and create a database service dbms. We will use a named volume pgdata for data persistence and use a mounted volume to enable communication between the data_importer container. The mounted volume will be mapped between the working directory of the data_importer app and the physical location in the local machine. This way, the system can take any data inside the local data directory and import to the SQL database automatically and securely. We will use a bridge network named telnet to enable container-to -container communication. lets save these parameters in a docker-compose.yml in the current working directory.\nservices :\n  dbms :\n    image : postgres:14\n    environment:\n      POSTGRES_DB: global_data\n      POSTGRES_USER: sqluser\n      POSTGRES_PASSWORD: secret\n    volumes :\n      - pgdata:/var/lib/postgresql/data\n      - ./data:/imported_data\n    ports :\n      - \"5432:5432\"\n    networks :\n      - telnet\n  \n  dataimporter:\n    image: data_importer\n    depends_on:\n      - dbms\n    networks:\n      - telnet\n    command: [\"01_paris_airbnb_listings.csv\",\"sqluser\",\"secret\",\"dbms\",\"5432\",\"global_data\",\"paris_airbnb_data\"]\n  \nvolumes :\n  pgdata :\n\nnetworks :\n  telnet :\n    driver : bridge"
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html#deployment",
    "href": "posts/2025-11-09-docker-data-pipeline.html#deployment",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "Deployment",
    "text": "Deployment\nThe complete ETL pipeline can be deployed using:\ndocker-compose up -d\nUpon successful execution, the data would be safely stored inside the global_data database in the dbms server. This can be verified using a docker exec command.\ndocker exec -it sql_dbms_1 psql -U sqluser -d global_data\nWhich yields\n\npsql (14.19 (Debian 14.19-1.pgdg13+1))  \nType \"help\" for help.  \n  \nglobal_data=# \\dt  \n             List of relations  \nSchema |       Name        | Type  |  Owner     \n--------+-------------------+-------+---------  \npublic | paris_airbnb_data | table | sqluser  \n(1 row)  \n  \nglobal_data=#\nNote that the table_name is correctly named as paris_airbnb_data contary to paris_airbnb as mentioned in the Dockerfile. This suggests that the data is correctly passed from the argument variables defined in the command operator in the docker-compose.yml."
  },
  {
    "objectID": "posts/2025-11-09-docker-data-pipeline.html#summary",
    "href": "posts/2025-11-09-docker-data-pipeline.html#summary",
    "title": "A Data Pipeline is a distributed system. Start Architecting It Like One",
    "section": "Summary",
    "text": "Summary\nIn this guide, we transformed a simple Python app into a production-ready data pipeline by adopting distributed systems principles.\n\nService Discovery: Containers communicate through Docker DNS (dbms) instead of hard-coded IPs.\nConfiguration Management: All parameters are externalized through CLI arguments and environment variables.\nNetwork Isolation: Secure bridge network enables container-to-container communication.\nData Persistence: Named volumes ensure database survival across container restarts.\nOrchestration: Docker Compose manages multi-service dependencies and lifecycle.\n\nTreating data pipelines as a distributed systems from the beginning eliminates the system breakdown across environments and creates infrastructure that scales from local development to production deployment.\nThe complete pipeline is available in GitHub."
  },
  {
    "objectID": "posts/2026-01-05-spark.html",
    "href": "posts/2026-01-05-spark.html",
    "title": "Essentials of Apache Spark",
    "section": "",
    "text": "Traditionally, processing big data was challenging because the sheer size of the data that exceeds far greater than the available compute resources. To handle such big data, Hadoop had “Hadoop Distributed File System (HDFS)” framework and batch processing through the MapReduce function. These innovations enabled distributed computing to run on commodity hardware. While this offered many advantages such as partitioned data, distributed processing etc, significant challenges remained in terms of scalability and speed since it is based on disc. This is because this way of distributed processing resulted in exponential read write cycles for each intermediate steps.\nSpark was developed to address these important bottlenecks in terms of speed and efficiency by adopting “in-memory computing”. This drasticially improved speed and efficiency of big data processing. There were many additional advantages in Spark which is detailed below."
  },
  {
    "objectID": "posts/2026-01-05-spark.html#apache-spark",
    "href": "posts/2026-01-05-spark.html#apache-spark",
    "title": "Essentials of Apache Spark",
    "section": "Apache Spark",
    "text": "Apache Spark\nApache Spark was developed by Matei Zaharia of University of California, Berkeley in 2009 and was donated to Apache Software Foundation in 2013.\nSpark is a distributed computing framework that addressed bottlenecks in the traditional Hadoop based big data tools. Spark achieves performance improvement on the following fronts:\n\nIn-memory computing\nPartitioning\nFault-tolerant computing\nUnified engine for big data ecosystem\n\n\n\n\nApache Spark\n\n\n\nIn-memory computing:\nSpark works via in-memory computing that retains the data cached in the memory. This alone can yield 100-1000x speedups. Imagine a computation with many steps. At each step, the data must be read from the disk, loaded into memory (RAM), processed, intermediate results are written back. This operation must be repeated until the operation is completed. Clearly, the bottleneck is the repeated read, write cycle of intermediary data to hard disk. Spark eliminates most of this repetitive I/O on the disk by keeping the intermediate results in memory, dramatically accelerating performance. Spark intelligently manages the memory by keeping maximum of the partition data in memory (RAM). When the partition data exceeds the allocated in-memory, spark spills over the least frequently used partitions to the physical memory (disk) and quickly read back if and when the partition is needed.\n\n\nAbstractions in Spark\nSpark developed its core abstraction called Resilient Distributed Datasets (RDD) in the 2010s followed by the now popular Dataset and DataFrame which are the latest (introduced 2015-2016) higher level API implementations of Spark. DataFrames and Datasets are the preferred APIs today for their optimizations and ease of use. The RDDs still remain functional and used for specialized use cases.\n\n\nData Partitioning:\nSpark partitions the data either by a variable in the data or arbitrarily by equal size. Partitioning by a variable of the data frame can be created using partitionby() method and arbitrary partitions can be created using repartition() method. Proper partitioning is critical for parallelism and optimized performance.\n\n\nFault-tolerant computing:\nSpark is fault tolerant since it can rapidly recreate the partition data on any failed nodes. Sparks tracks the lineage of the partition from the original data. If a node fails, spark rebuilds the lost data without ever stopping or restarting the process. This enabled horizontal scaling to handle petabytes of data reliably.\n\n\nSpark has an Unified Engine\nTraditional Hadoop based solutions relied on different independent frameworks for specific processing of the data. For example for bigdata processing, there was MapReduce, for Stream Processing there was Strom or Samza, for SQL queries there was Hive or Impala. These resulted in a data silos, complex architectures and expensive data overheads. This is because the engineers had to master many tools and integrate into their system. Spark has the core engine and has several higher level APIs such as Spark SQL, Structured Streaming, Spark MLlib and Spark Graphx for querying, real-time data processing, machine learning and graph analytics all unified in the Spark framework, making it a powerful modern data stack.\n\n\nSpark is Polyglot\nSpark’s core is written in Scala and APIs for Python (pyspark), Java, R (sparklyr) and SQL are available making it a powerful language across different data and statistical stacks. Spark documentation describes near identical performance across these languages.\n\n\nConclusion\nApache Spark revolutionized big data processing by providing a unified, in-memory computing framework that addressed the limitations of traditional Hadoop ecosystems. By combining in-memory processing, intelligent partitioning, fault tolerance, and a polyglot API approach, Spark delivers dramatic performance improvements while simplifying complex data architectures. Its ability to handle diverse workloads from batch processing and real-time streaming to machine learning and graph analytics makes it a powerful engine for today’s big data challanges. All these operations are efficiently performed through a single integrated engine makes it a cornerstone of modern data platforms. As data volumes continue to grow and processing demands become more complex, Spark’s balanced approach of performance, reliability, and developer accessibility ensures its enduring relevance in the data engineering landscape."
  },
  {
    "objectID": "posts/2026-01-14-spark-architecture.html",
    "href": "posts/2026-01-14-spark-architecture.html",
    "title": "Essentials of Spark Architecture",
    "section": "",
    "text": "Apache Spark is originally built on RDD abstraction and it is constructed in a well defined layered architecture. DataFrames and Datasets are used in modern spark applications.\nIn its core, Spark has three main components."
  },
  {
    "objectID": "posts/2026-01-14-spark-architecture.html#driver-program",
    "href": "posts/2026-01-14-spark-architecture.html#driver-program",
    "title": "Essentials of Spark Architecture",
    "section": "Driver Program:",
    "text": "Driver Program:\nAny script that uses spark to process big (and not so big) data is a spark application. The Driver program is the entry point of the application and it is where the main script is hosted.\nThe driver program reads the script and creates a logical plan of operations Directed Acyclic Graph. In spark, there are two types of operations. Transformations and actions. The DAG is comprised of these transformations and actions. The smallest sub unit of any operation is a task and a set of tasks are grouped in a stage. Spark DAG is built according to the logic of the user’s application. The logical DAG is broken down into stages and TaskScheduler serializes and assigns individual tasks. The Driver program stores the metadata of the tasks shared with the executors and the results are collected back once the tasks are completed. The taks lineage is readily available in the driver so that it can quickly rebuild a lost partition in case a failure. The process of building serialized stages is called DAG scheduling. The scheduled DAG stages are then transferred to Task Scheduler. TaskScheduler assigns tasks to executors. Actual execution happens only when an action is required, this is the so called ‘Lazy Evaluation’."
  },
  {
    "objectID": "posts/2026-01-14-spark-architecture.html#sparksession-and-sparkcontext",
    "href": "posts/2026-01-14-spark-architecture.html#sparksession-and-sparkcontext",
    "title": "Essentials of Spark Architecture",
    "section": "SparkSession and SparkContext",
    "text": "SparkSession and SparkContext\n\nSparkSession\nSparkSession is the latest implementation in Spark v&gt;2.x.x. The Driver program creates SparkSession which is the entry point of a program. In SparkSession, along with SparkContext, SQL, Streaming, ML and Graph context are available and unified into one engine under the same SparkSession offering superior flexibility. SparkContext is now available as backward compatible to access low level RDD features. Since the higher level APIs are accessible with SparkSession, Apache recommends to initialize with SparkSession.\n\n\nSparkContext\nSparkContext is created by the driver program and this was formerly the entry point until spark 2.x.x. SparkContext connects to the cluster manager and acquires the required resources for the distributed computation. It then sends the serialized DAG stages to the executor nodes and keeps track of the execution via regular heart beat messages.\n\n\nCluster manager :\nThe computing resources are managed by the cluster manager. There are three different types of cluster management in Spark. Namely, Spark standalone, Kubernetes and YARN. Often the resource manager resides in master node but this depends on the deployment mode of the application. In local deployment mode, The driver and executor nodes are hosted in a single thread and spark manages the threads itself and no cluster manager is involved in this case.\nThe resource manager receives requests from the task scheduler and launches executor processes on the worker nodes. The executor process(es) are launched in pods (kubernetes), containers /raw processes (spark standalone) depending on the configuration of the deployment. It then pings back the availability of the worker nodes and a connection is established between the spark driver program and the worker nodes.\n\n\nExecutors\nThe executor manages the task execution and manages the memory, it communicates with the driver program to report the status of the assigned tasks and return the computed results. The executors are also involved in shuffle operations and act as a server for other executors to access the local data of the executor."
  },
  {
    "objectID": "posts/2026-02-15-7s-principle-and-data-warehousing.html",
    "href": "posts/2026-02-15-7s-principle-and-data-warehousing.html",
    "title": "Data Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective",
    "section": "",
    "text": "Back when I was working as a production engineer in India, the company had implemented the concept of “7S-System of House Keeping”. The 7S is an extension of the Japanese philosophy of 5S.\nThe components of 7S are:\nNow working in the domain of data, I often think about this philosophy and how intertwined it is with the data warehousing. Although the factory warehouse and Data warehouse different purposes, the fundamental actions are the same.\nFor example, a factory warehouse\nA Data warehouse follows the same logic. It receives incoming data either in batches or in real-time streams, the incoming data is handled in the staging area, for their correctness, missing values and normalization. Once they are compliant with the specification, the facts and dimensions are modelled and stored in appropriate tables in the database for the end users to consume.\nHere is a breakdown of how the 7S philosophy directly applies to a modern data warehouse:\nWhile there is an order for the 7S principles, they are not necessarily sequential. That is to say, it does not follow the same order as the data warehousung process. However, the principles are all relevant and can be applied at different stages of the data warehousing process."
  },
  {
    "objectID": "posts/2026-02-15-7s-principle-and-data-warehousing.html#seiketsu---clean-to-achieve-consistency",
    "href": "posts/2026-02-15-7s-principle-and-data-warehousing.html#seiketsu---clean-to-achieve-consistency",
    "title": "Data Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective",
    "section": "Seiketsu - Clean to achieve consistency",
    "text": "Seiketsu - Clean to achieve consistency\nThe Seiketsu principle emphasizes the importance of cleanliness and organization. In the context of data warehousing, this can be interpreted as the need for data cleansing and data quality management. Just like in a factory warehouse, where dirty or damaged items can cause problems, in a data warehouse, dirty or inaccurate data can cause problems for end users. This could be achieved through the use of data cleansing techniques such as data profiling, data validation, and data transformation, which help to ensure that the data is accurate and consistent. Additionally, it also emphasizes the importance of monitoring and maintaining the quality of the data over time, which helps to ensure that the data remains accurate and useful for end users."
  },
  {
    "objectID": "posts/2026-02-15-7s-principle-and-data-warehousing.html#seiri-and-seiton---orderliness",
    "href": "posts/2026-02-15-7s-principle-and-data-warehousing.html#seiri-and-seiton---orderliness",
    "title": "Data Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective",
    "section": "Seiri and Seiton - Orderliness",
    "text": "Seiri and Seiton - Orderliness\nThe Seiri and Seiton principles are not dramatically different from each other. They both emphasizes the importance of organization and order. In the context of data warehousing, this can be interpreted as the need for a well-designed data model and schema. Just like in a factory warehouse, where each item has a specific place, in a data warehouse, each piece of data should have a specific place in the database/ schema or even in table. This could be achieved through the use of dimensional modeling techniques such as star schema or snowflake schema, which help to organize the data in a way that is easy to understand and query. Additionally, it also emphasizes the importance of metadata management, which helps to ensure that the data is properly documented and can be easily understood by end users."
  },
  {
    "objectID": "posts/2026-02-15-7s-principle-and-data-warehousing.html#seiso---stratification",
    "href": "posts/2026-02-15-7s-principle-and-data-warehousing.html#seiso---stratification",
    "title": "Data Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective",
    "section": "Seiso - Stratification",
    "text": "Seiso - Stratification\nThe Seiso principle also emphasizes the importance of stratification, which is the process of organizing items into different layers or levels. In the context of data warehousing, this can be interpreted as the need for a well-designed data architecture that separates different types of data into different layers. For example, a common architecture for a data warehouse is to have a staging layer, where raw data is stored, a presentation layer, where cleaned and transformed data is stored, and a consumption layer, where data is made available for end users. It can also be compared to the popular medallion architecture, which separates data into bronze, silver, and gold layers.\nThis separation of layers helps to ensure that the data is properly organized and can be easily accessed by end users. While the Seiri and Seiton principles also emphasize the importance of organization, the Seiso principle specifically emphasizes the importance of stratification, that stratify the process with the operation layers in terms of architecture which is a crucial aspect of data warehousing."
  },
  {
    "objectID": "posts/2026-02-15-7s-principle-and-data-warehousing.html#shitsuke---training-and-team-culture",
    "href": "posts/2026-02-15-7s-principle-and-data-warehousing.html#shitsuke---training-and-team-culture",
    "title": "Data Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective",
    "section": "Shitsuke - Training and Team culture",
    "text": "Shitsuke - Training and Team culture\nThe Shitsuke principle emphasizes the importance of training and team culture. In the context of data warehousing, this can be interpreted as the need for a well-trained team that is familiar with the data warehousing process and the tools and technologies used to build and maintain the data warehouse. This could be achieved through regular training sessions, workshops, and knowledge sharing among team members. Additionally, it also emphasizes the importance of fostering a culture of collaboration and continuous learning within the team, which helps to ensure that the team is able to adapt to changes in technology and business requirements over time."
  },
  {
    "objectID": "posts/2026-02-15-7s-principle-and-data-warehousing.html#shikkari---standardization",
    "href": "posts/2026-02-15-7s-principle-and-data-warehousing.html#shikkari---standardization",
    "title": "Data Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective",
    "section": "Shikkari - Standardization",
    "text": "Shikkari - Standardization\nShikkari is the principle that emphasizes the importance of standardization. In the context of data warehousing, this can be interpreted as the need for a well-defined data warehousing process, strict adherence to defined data contracts. It is important to have a clearly defined schema, data quality rules, Service Level Agreements (SLAs) and data governance policies in place to ensure that the data is accurate and consistent."
  },
  {
    "objectID": "posts/2026-02-15-7s-principle-and-data-warehousing.html#shitsukoku---build-for-persistence",
    "href": "posts/2026-02-15-7s-principle-and-data-warehousing.html#shitsukoku---build-for-persistence",
    "title": "Data Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective",
    "section": "Shitsukoku - Build for persistence",
    "text": "Shitsukoku - Build for persistence\nThis last principle advocates that the system must be built for durability and persistence. In the context of data warehousing, this can be interpreted as the need for a well-designed data warehousing architecture that is scalable, reliable, and maintainable. Notably, the data engineers must build the data warehouses that ensures resillience by anticipating potential issues and implementing appropriate measures to mitigate them. For example, this could include a systematic approach to data backup and recovery, version control for data and code,and proactive technical debt management.\n\n\n\nAI summarized infographic of the 7S principles applied to data warehousing"
  },
  {
    "objectID": "posts/2026-02-15-7s-principle-and-data-warehousing.html#concluding-thoughts",
    "href": "posts/2026-02-15-7s-principle-and-data-warehousing.html#concluding-thoughts",
    "title": "Data Warehousing and Japanese Philosophy of Housekeeping: A Unique Perspective",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\nBy adopting a datawarehouse design based on the 7S philosopy, data engineers can build, operate and maintain a sustainable, high-quality and well-governed data warehouse that serves the needs of the business and end users effectively. The 7S principles provide a holistic framework for thinking about the various aspects of data warehousing, from data quality and organization to team culture and architecture."
  }
]